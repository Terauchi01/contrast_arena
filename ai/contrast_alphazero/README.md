# コントラスト (Contrast) - AIボードゲーム

5x5の盤面で、タイルの色によって移動方向が変化する戦略的な2人対戦ボードゲームです。

## ゲームルール

### 基本情報
- **盤面**: 5x5グリッド
- **プレイヤー**: 2人
- **各プレイヤーのコマ**: 5個
- **持ちタイル**: 黒タイル3枚、グレータイル1枚
- **ターン制**: 交互に1手ずつ行動、駒を動かした後にタイルを配置可能
- **移動制限**: 相手のコマがいるマスには移動不可、自分の駒が隣にある場合、その駒を飛び越えて移動可能
- **タイル制限**: タイルの再配置は認められない
- **勝利条件**: 相手の陣地（後列）に最初に到達
- **敗北条件**: 自分の駒が全て動けなくなった場合

### タイルシステム
- **白タイル** (□): 縦横方向に1マス移動可能
- **黒タイル** (■): 斜め方向に1マス移動可能
- **グレータイル** (▦): 全8方向に1マス移動可能

### 初期配置
```
   0  1  2  3  4
0 [2][2][2][2][2]  ← Player 2 (ゴールはy=4)
1 [ ][ ][ ][ ][ ]
2 [ ][ ][ ][ ][ ]
3 [ ][ ][ ][ ][ ]
4 [1][1][1][1][1]  ← Player 1 (ゴールはy=0)
```

### ゲームの流れ
1. 各ターン、プレイヤーは以下を実行:
   - (オプション) 白タイルを黒/グレータイルに変更
   - 自分のコマを1つ移動
2. **相手のコマがいるマスには移動できません**
3. タイル配置数: 黒タイル×3、グレータイル×1 (各プレイヤー)

## プロジェクト構成

```
contrast_alphazero/
├── config.py              # 設定ファイル（ハイパーパラメータ、定数）
├── contrast_game.py       # ゲームロジック
├── model.py              # ニューラルネットワークモデル
├── mcts.py               # モンテカルロ木探索の実装
├── main.py               # 学習メインループ
├── elo_evaluator.py      # ELOレーティング評価
├── rule_based_ai.py      # ルールベースAI
├── play_vs_ai.py         # 対戦インターフェース
├── logger.py             # ロギング設定
├── debug.py              # デバッグツール
├── logs/                 # ログファイル
├── models/               # 保存されたモデル
└── tests/                # テストコード
```

## 使い方

### 1. 学習済みモデルと対戦
```bash
# デフォルト設定で対戦（人間 vs AI）
uv run play_vs_ai.py

# オプション指定
uv run play_vs_ai.py --model contrast_model_final.pth --simulations 200

# AI同士の対戦
uv run play_vs_ai.py --player1 ai --player2 rule

# ヘルプ表示
uv run play_vs_ai.py --help
```

#### 対戦オプション
- `--model`: モデルファイルパス（デフォルト: `contrast_model_final.pth`）
- `--simulations`: MCTSシミュレーション回数（デフォルト: 100）
- `--player1`: プレイヤー1のタイプ（`human`, `ai`, `random`, `rule`）
- `--player2`: プレイヤー2のタイプ（`human`, `ai`, `random`, `rule`）

#### 操作方法
- **入力形式**: `<移動前>,<移動後> <配置座標><タイルカラー>`
  - 例: `b1,b2 b3g` → b1からb2へ移動、b3にグレータイル配置
  - 例: `c5,c4` → c5からc4へ移動（タイル配置なし）
- **座標**: 列(a-e)と行(1-5)で指定
- **タイルカラー**: `b`(黒), `g`(グレー)

### 2. AIを学習させる
```bash
# 学習実行
uv run main.py

# デバッグモード
uv run debug.py
```

#### 学習の監視
- ログファイル: `logs/training_YYYYMMDD_HHMMSS.log`
- モデル保存: `models/model_step_<step>_elo_<elo>.pth`
- 最終モデル: `contrast_model_final.pth`

### 3. 設定のカスタマイズ

`config.py`で以下のパラメータを調整できます：

```python
# MCTS設定
NUM_SIMULATIONS = 50        # シミュレーション回数
DIRICHLET_ALPHA = 0.3       # 探索ノイズ
C_PUCT = 1.0               # 探索バランス

# 学習設定
BATCH_SIZE = 128           # バッチサイズ
LEARNING_RATE = 0.2        # 学習率
BUFFER_SIZE = 20000        # リプレイバッファサイズ

# 評価設定
EVAL_INTERVAL = 1000       # 評価間隔
EVAL_NUM_GAMES = 500       # 評価時の対戦数
```

## セットアップ

### 必要要件
- Python 3.8+
- PyTorch 2.0+
- NumPy
- Ray (並列処理用)

### インストール (uv使用)
```bash
# uvで依存関係をインストール
uv sync
```

### 3. テストを実行
```bash
# 全テストを実行
uv run -m pytest tests -v

# 特定のテストのみ
uv run -m pytest tests/test_contrast_game.py -v
uv run -m pytest tests/test_mcts.py -v
```

## モデル構成

### ネットワークアーキテクチャ

**ContrastDualPolicyNet** - マルチヘッド深層ニューラルネットワーク

```
入力: (90, 5, 5) テンソル
  ├─ 0-7チャンネル: 自分の駒位置（履歴8手分）
  ├─ 8-15チャンネル: 相手の駒位置（履歴8手分）
  ├─ 16-23チャンネル: 黒タイル位置（履歴8手分）
  ├─ 24-31チャンネル: グレータイル位置（履歴8手分）
  ├─ 56-63チャンネル: 自分の黒タイル数（履歴8手分）
  ├─ 64-71チャンネル: 自分のグレータイル数（履歴8手分）
  ├─ 72-79チャンネル: 相手の黒タイル数（履歴8手分）
  ├─ 80-87チャンネル: 相手のグレータイル数（履歴8手分）
  ├─ 88チャンネル: 現在のプレイヤーID
  └─ 89チャンネル: 手数（正規化済み）

↓ 畳み込み層 (3x3, 64フィルタ) + BatchNorm + ReLU

↓ Residualブロック × 4
  └─ 各ブロック: Conv3x3 → BN → ReLU → Conv3x3 → BN → Skip接続 → ReLU

         ┌─────────────┬─────────────┬─────────────┐
         │             │             │             │
    Move Head     Tile Head     Value Head
         │             │             │
    Conv1x1(32)   Conv1x1(16)   Conv1x1(4)
         │             │             │
    BN + ReLU     BN + ReLU     BN + ReLU
         │             │             │
    Flatten       Flatten       Flatten
         │             │             │
    FC(625)       FC(51)        FC(32) → ReLU
         │             │             │
         │             │         FC(1) → Tanh
         │             │             │
    Move Logits   Tile Logits   Value [-1, 1]
    (625次元)     (51次元)      (1次元)
```

#### 出力の意味

1. **Move Head (625次元)**
   - From位置(25) × To位置(25) = 625通りの移動パターン
   - Softmax適用後、各移動の確率分布を表現

2. **Tile Head (51次元)**
   - Pass(1) + 黒タイル配置(25) + グレータイル配置(25) = 51通り
   - Softmax適用後、タイル配置の確率分布を表現

3. **Value Head (1次元)**
   - 現在の局面評価値 [-1.0, 1.0]
   - +1.0: 現在のプレイヤーが有利
   - -1.0: 相手プレイヤーが有利
   - 0.0: 互角

### ハイパーパラメータ（デフォルト）

| パラメータ | 値 | 説明 |
|-----------|-----|------|
| **ネットワーク** | | |
| Residualブロック数 | 4 | 特徴抽出の深さ |
| フィルタ数 | 64 | 畳み込み層のチャンネル数 |
| Move Head フィルタ | 32 | 移動ヘッドの中間層 |
| Tile Head フィルタ | 16 | タイル配置ヘッドの中間層 |
| Value Head フィルタ | 4 | 価値ヘッドの中間層 |
| **学習** | | |
| バッチサイズ | 1024 | 1回の更新で使用するサンプル数 |
| リプレイバッファ | 20,000 | 保存する過去の経験数 |
| 学習率 | 0.001 | Adam最適化の初期学習率 |
| 学習率減衰 | 50,000ステップごとに0.5倍 | |
| 重み減衰 | 1e-4 | L2正則化係数 |
| **MCTS** | | |
| シミュレーション回数 | 50 | 1手あたりの探索回数 |
| C_PUCT | 1.0 | 探索と活用のバランス係数 |
| ディリクレα | 0.3 | ルートノードのノイズ |
| ディリクレε | 0.25 | ノイズの混合比率 |
| 温度閾値 | 30手 | この手数まで確率的に選択 |
| **評価** | | |
| 評価間隔 | 100ステップ | 学習中の評価頻度 |
| 評価ゲーム数 | 10 | 1回の評価での対戦数 |
| ベースラインELO | 1000 | RuleBasedAIの初期ELO |

### 損失関数

AlphaZeroスタイルの複合損失:

```
Total Loss = Value Loss + Move Policy Loss + Tile Policy Loss

Value Loss = MSE(predicted_value, actual_outcome)
Move Policy Loss = -Σ(MCTS_policy_move × log(NN_policy_move))
Tile Policy Loss = -Σ(MCTS_policy_tile × log(NN_policy_tile))
```

- **Value Loss**: 実際のゲーム結果との平均二乗誤差
- **Policy Loss**: MCTSの探索結果を教師とした交差エントロピー損失

## 実装の特徴

### リファクタリング内容
1. **定数管理**: `config.py`で一元管理
2. **型ヒント**: 全関数に型アノテーションを追加
3. **ドキュメント**: 詳細なdocstringを追加
4. **ログ出力**: 学習プロセスの詳細な追跡
   - 損失の内訳（Value, Move, Tile）
   - MCTSの統計情報
   - 評価結果（ELO、勝率）
5. **エラーハンドリング**: 各段階でのバリデーション強化

### ログ情報
学習中に以下の情報がログに記録されます：
- **学習メトリクス**: 総損失、価値損失、移動損失、タイル損失
- **Self-play情報**: ゲーム結果、手数、MCTS統計
- **評価結果**: ELOレーティング、勝率、モデル保存パス
- **システム情報**: バッファサイズ、学習率、デバイス情報

## テスト
### テストカバレッジ

プロジェクトには **86個** の網羅的なテストが含まれています。

#### ContrastGame テスト (47テスト)

**1. 初期化と初期配置 (5テスト)**
- 初期盤面の配置確認
- 初期タイル（全て白）の確認
- 持ちタイル数（黒3、グレー1）の確認
- 最初のプレイヤーの確認
- ゲーム開始状態の確認

**2. 移動ルール (8テスト)**
- 白タイル：縦横移動のテスト
- 黒タイル：斜め移動のテスト
- グレータイル：8方向移動のテスト
- 相手の駒によるブロックのテスト
- 味方の駒を飛び越える動作のテスト
- 盤外への移動不可のテスト
- 相手の駒を動かせないことのテスト
- 複数の味方駒の連続飛び越えのテスト

**3. タイル配置ルール (5テスト)**
- 黒タイル配置の動作確認
- グレータイル配置の動作確認
- 移動先への配置不可の確認
- 既存コマ位置への配置不可の確認
- タイル使い切り後の動作確認

**4. 勝利条件 (3テスト)**
- P1が上段到達で勝利することの確認
- P2が下段到達で勝利することの確認
- 勝者なしでゲーム継続の確認

**4.5. 敗北条件 (2テスト)** ⭐新規追加
- 合法手がない場合に敗北することの確認
- 相手の行動後に合法手がなくなり敗北することの確認

**5. アクションエンコーディング (4テスト)**
- アクションのエンコード基本動作
- アクションのデコード基本動作
- エンコード↔デコードの往復変換
- ハッシュ値の範囲確認

**6. 合法手生成 (4テスト)**
- 初期状態での合法手存在確認
- ゲーム終了時の合法手なし確認
- タイルなし移動の含有確認
- 全合法手の実行可能性確認

**7. ゲームコピー (2テスト)**
- 独立したインスタンス作成の確認
- 状態の正確な保存の確認

**8. 状態エンコーディング (4テスト)**
- テンソル形状 (90, 5, 5) の確認
- データ型 (float32) の確認
- 履歴パディングの確認
- 値の範囲の確認

**9. 履歴管理 (3テスト)**
- 履歴の初期化確認
- ステップごとの更新確認
- 最大長8の維持確認

**10. エッジケース (3テスト)**
- 全駒ブロック状態の処理（合法手が0になることを確認）
- ゲームリセット機能
- プレイヤー切り替えマッピング

**11. 複雑なシナリオ (4テスト)**
- 完全なゲームシミュレーション
- プレイヤー交互切り替えの確認
- タイル配置による移動変化の確認
- アクション一貫性の確認

#### MCTS テスト (39テスト)

**1. MCTS初期化 (3テスト)**
- 基本的な初期化の確認
- カスタムパラメータでの初期化
- 空の辞書状態の確認

**2. ゲーム状態キー生成 (4テスト)**
- 決定的なキー生成の確認
- 異なる状態で異なるキー生成の確認
- move_countの含有確認
- プレイヤー情報の含有確認

**3. ノード展開 (6テスト)**
- エントリ作成の確認
- 値の初期化確認
- 評価値の返却確認
- 合法手のみ展開の確認
- 確率の合計が1であることの確認
- 終了状態の展開確認

**4. 探索 (5テスト)**
- ポリシー返却の確認
- 確率合計が1であることの確認
- 複数シミュレーションの実行確認
- ディリクレノイズ付加の確認
- 終了状態で空返却の確認

**5. 評価 (5テスト)**
- 終了状態（勝者）の評価値確認
- 終了状態（敗者）の評価値確認
- 終了状態（引き分け）の評価値確認
- 未展開ノードの展開確認
- 統計の更新確認

**6. PUCT (2テスト)**
- 未訪問アクション優先の確認
- 探索と活用のバランス確認

**7. バックプロパゲーション (2テスト)**
- 訪問回数の更新確認
- 価値の更新確認

**8. エッジケース (5テスト)**
- シミュレーション回数0の処理
- シミュレーション回数1の処理
- 合法手なし状態の処理
- 同一ツリーでの複数探索
- 元のゲーム状態の非変更確認

**9. 実際のネットワークとの統合 (2テスト)**
- 実ネットワークでの動作確認
- ポリシーの合法性確認

**10. 統合テスト (3テスト)**
- 完全なゲームシミュレーション
- シミュレーション回数による改善確認
- ゲーム状態遷移の処理確認

**11. 価値の伝播 (2テスト)**
- 再帰中の価値反転確認
- 終了価値の伝播確認

### テスト実行結果

```bash
$ uv run -m pytest test_contrast_game.py test_mcts.py -v
============================== 84 passed in 1.30s ==============================
```

全84テストが成功し、ゲームロジックとMCTSアルゴリズムの正確性が保証されています。
